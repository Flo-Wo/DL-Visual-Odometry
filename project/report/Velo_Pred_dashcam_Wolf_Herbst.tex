\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
%% own packages
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
\usepackage[capitalise]{cleveref}
\usepackage{csquotes}
\usepackage{blindtext}
\usepackage{enumitem}

\usepackage[backend=biber, style=numeric, sorting=none]{biblatex} %Zitate mit BibTeX  %, sorting=none "richtige"
\addbibresource{ML-literature.bib}

% Reihenfolge der Nummern
%, citestyle=alphabetic-verb

\begin{document}

\title{A machine learning approach to predict a vehicles velocity using dashcam video}

\author{\IEEEauthorblockN{Florian Wolf}
\IEEEauthorblockA{\textit{Department of Mathematics and Statistics, University Konstanz}\\
Konstanz, Germany\\
florian.2.wolf@uni-konstanz.de}
\and
\IEEEauthorblockN{Franz Herbst}
\IEEEauthorblockA{\textit{Department of Physics, University Konstanz} \\
Konstanz, Germany\\
franz.herbst@uni-konstanz.de}
}

\maketitle

\begin{abstract}
\blindtext[2]
\end{abstract}

\begin{IEEEkeywords}
deep learning, computer vision, visual odometry, dense optical flow, siamese network
\end{IEEEkeywords}

\section{Introduction}

Autonomous driving is considered to have a key role in the future of human mobility. Recently the topic has therefore gained great attention in research, economy and politics \cite{Maurer2016}. In this project we approached this topic in a related but more simplified setup and tried out several machine learning techniques and network architectures.

\subsection{Aim of the project}
\label{subsec:AimAndMeasure}

In this project we tried to predict the velocity of a car based on a dashcam video. This was inspired by  the \emph{comma}\footnote{\url{https://comma.ai/}} speed challenge\footnote{\url{https://github.com/commaai/speedchallenge}} published in 2018. To achieve this goal we used optical flow analysis to preprocess the data and neuronal networks with different structures (classical and siamese) to predict the velocity. In order to measure the quality of the predictions we used their mean squared errors of the measured velocity.

\section{Data collection, analysis and preprocessing}

To start the project we used the database given in the \emph{comma} speed challenge project, which consists of a 17 minute training video (20400 frames) and the corresponding car velocity, as well as a 9 minute test video without labels to validate the model on an unknown data set.

\subsection{Data collection}

Due to our limited computational power we mainly used this data set to study different training techniques and model architectures. Still we could not expect good generalization for this rather small pool of training data. We therefore developed a technique to acquire more data that needed minimal resources. We used the open source smartphone apps \emph{open camera} and \emph{open street maps} to cast a video while driving and map the velocity using GPS at the same time. So we were able to train our final model with 1 hour 40 minutes of video data.

\subsection{Data analysis}

In order to evaluate our raw data with the model we analysed our dataset concerning the recorded images and driving scenarios. The original frames have a size of $640\times380\times3$ pixels (RGB). To reduce the computation time, we cut the borders of the frames to exclude parts not needed for detection and sampled them down to half of their pixel size.

To visualize the training process in different driving scenarios, we plotted the velocity in dependence of the frame number (\cref{fig:SpeedPerFrameDistributionNewSplitting}). After additional evaluation of the video we were able to classify three different driving scenarios: \emph{highway driving} with high and relatively steady velocities, \emph{stop and go} with low and fluctuating velocities as well as \emph{city driving} with very abrupt speed changes between medium and very low velocities. Every driving scenario is represented with about one third of the training data.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.55]{./imgs/plot_speed_time_new_splitting.eps}
	\caption{Distribution of the velocities of all frames in the training video, including the three categories.}
	\label{fig:SpeedPerFrameDistributionNewSplitting}
\end{figure}

We would then evaluate the predictions of each model $p(x_i)$ to the input data $x_i$ using the mean squared error
\begin{align}
	\mathcal{L} = \frac{1}{N} \sum\limits_{i=1}^{N} (p(x_i) - y_i)^2
\end{align}
to real value of the velocity $y_i$. To classify the resulting value, we used the velocity plot and the 
following rough classification\footnote{These rules do only apply on datasets with an equal distribution 
of all driving scenarios}:
\begin{table}[h!]
\normalsize
\centering
\begin{tabular}{r l}
$\sqrt{\mathcal{L}} \gtrsim 16$: & no fitting\\
$16 \gtrsim \sqrt{\mathcal{L}} \gtrsim 10$: & average velocity fitted\\
$10 \gtrsim \sqrt{\mathcal{L}} \gtrsim 5$: & scenarios detected\\
$5 \gtrsim \sqrt{\mathcal{L}} \gtrsim 1$: & variances within scenarios detected\\
$1 \gtrsim \sqrt{\mathcal{L}}$: & perfect fitting
\end{tabular}
\caption{Classification of the result based on the square root of the MSE.}
\end{table}

%\begin{itemize}
%\item $\sqrt{\mathcal{L}} \gtrsim 16$: \textit{no fitting}
%\item $16 \gtrsim \sqrt{\mathcal{L}} \gtrsim 10$: \emph{average velocity fitted}
%\item $10 \gtrsim \sqrt{\mathcal{L}} \gtrsim 5$: \emph{scenarios detected}
%\item $5 \gtrsim \sqrt{\mathcal{L}} \gtrsim 1$: \emph{variances within scenarios detected}
%\item $1 \gtrsim \sqrt{\mathcal{L}}$: \emph{perfect fitting}
%\end{itemize}
For the training process we used a splitting of 80\% training data and 20\% validation data. Initially 
we did a hard splitting of the entire data, but this neglects the distribution of different driving 
scenarios in the video. Therefore we went on first splitting the data into the driving scenarios with 
then assigning training data and validation data on each.

\subsection{Preprocessing}

After the previously explained preparation steps, we needed to find a method to extract information 
about the motion of the vehicle to be able to predict its velocity. There are two ways of doing this: 
either fit two following frames into a network, which will be discussed later, or calculate the relative 
motion, the so-called \emph{optical flow}, between two frames and fit the results into a model.

We used the \enquote{Farneback pyramid method} \cite{Farneback2003} with  the following parameters to 
calculate the optical flow
\begin{align*}
\text{pyramid levels} &:= 3\\
\text{pyramid scaling} &:= 0.5\\
\text{window size} &:= 6\\
\text{pixel neighborhood size} &:= 5\\
\text{SD of the gaussian filter} &:= 1.1.
\end{align*}
We choose three pyramid levels, because we wanted the calculations to be more accurate. To decrease the 
training duration, we halved the size of the optical flow frames again, resulting in a resolution of
$(160,105,3)$ pixels per frame. As we used a window size of six pixels, a comparison between the 
original optical flow and the down sampled one lead to the result, that we do not loose a lot of 
information.

The optical flow calculation returns the magnitude and
the angle of the flow vectors (overall a two-dimensional vector field), which we transformed into polar 
coordinates.

To get an RGB image representing the
optical flow of two consecutive frames, we normalized the magnitudes and put them into the third channel 
of the frame. The values
of the second channel were all set to the value $255$. We then multiplied the angle with the factor 
$180/(2\pi)$ and set this value 
for the first channel.

We wanted to see, if the model performs better using the dashcam frames as additional material. 
Therefore, we did the same down sampling with the raw frames.

\section{Method selection and architecture}
The prediction of the vehicles speed is a non-linear regression task, so the choice of a neural network 
is reasonable. Recent architectures (ResNet, GoogLeNet, etc.) have shown that using multiple stacked 
convolution layers combined with stacked dense layers, perform well on image classification tasks. 
Therefore the choice of a convolutional neural network is justified.

\subsection{Initial Network}

As an initial architecture we used the model presented by the \emph{NVIDIA} work group in 
\cite{NVIDIA2016}.This network was designed for self-driving cars, so the model has enough complexity to 
handle a task like ours and allowed various possibilities to fine-tune and improve the architecture. The 
raw structure is shown in \cref{fig:initialNetwork}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\columnwidth]{imgs/InitialNetwork.png}
	\caption{Initial Network using a series of convolutional layers which are then flattened and }
	\label{fig:initialNetwork}
\end{figure}

Using the initial model on the training data with a hard splitting, we achieved a MSE of 18-20 in the validation set, while having a MSE of less than 3 on the training set. The initial network therefore has some clear overfitting problems, which we tried to address with our fine tuning.

\subsection{Siamese Network}

In order to improve our results we also tried to expanded our model and use the original image as well. The corresponding model is shown in \cref{fig:siameseNetwork} and inspired by the architecture used in
\cite{Wang2017}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\columnwidth]{imgs/siamese_model.png}
	\caption{Siamese Network}
	\label{fig:siameseNetwork}
\end{figure}

We could use this architecture in two different ways: feeding two consecutive frames into the model or one image and one optical flow field. In order to keep the structure correspondent we sampled down the regarding images to the size of the optical flow field.

\section{Fine tuning}

\subsection{Batch normalisation and activation functions}
As we did in the lecture, we included batch normalization layers \cite{BatchNorm2015}, to speed up
the training and improve the networks performance.

We also tested different activation functions. Like proposed in the lecture, we initially used the ReLu
function
\begin{align*}
\mathrm{ReLu}: \mathbb{R} \to \mathbb{R}_0^+, x \mapsto \max\{0,x\}.
\end{align*}
Using the ReLU function and 15 epochs for training, we achieved a MSE of around 15 on the 
testing set. We ran the code multiple times, to ensure this result holds. This result was not really 
promising, so we wanted to decrease the error by modifying the model even more.

To solve the problem of dead neurons\footnote{One can clearly see in the definition of the ReLU 
function, that neurons with a value below zero cannot participate in the learning process.} of the ReLU function, we 
tried the leakyReLU function
\begin{align*}
\mathrm{leakyReLU} : \mathbb{R} \to \mathbb{R}, x \mapsto \begin{cases}
x, x \geq 0\\
c \cdot x, x <0
\end{cases}
\end{align*}
with a hyperparameter $c = 0.01$. Using the leakyReLU function, we achieved a MSE of around 12 on the 
testing set and under 3 on the training set. All of the models were trained with only eight epochs, as 
we still had problems with overfitting the data.

\subsection{Dropout Layer}
As we still had a lot of overfitting issues, we decided to include a dropout layer, according to 
\cite{Dropout2014}. We tried different positions and different amounts of dropout layers, but using one 
layer with a probability of $p=0.5$ after the third convolutional layer seemed to work best.

\subsection{Pooling layers with initial splitting}
We added two generic pooling layers to the network to reduce the number of parameters of the model. One after the second 
convolutional layer and the second one right before the fully connected layers start\footnote{Indeed, the number of 
parameters decreased from a total of 636.225 trainable parameters to 156.225. Therefore the number of parameters decreased by a 
factor of 4. We calculated these numbers using the tool \enquote{PyTorch summary} (\url{https://github.com/sksq96/pytorch-summary}).}.We tested maximum and average pooling with the following 
parameters
\begin{align*}
\text{kernel size} &:= 2\times 2\\
\text{stride} &:= 2\\
\text{padding} &:= 1\\
\text{dilatation} &:= \text{None}
\end{align*}
The implementation of pooling layers helped a lot, as now the loss on the train and test data seemed to 
decrease nearly equally. Our results with the initial splitting are shown 
\cref{tab:ResultsInitialSplitting}. We gave the network
with max pooling a try with 15 epochs, as the loss on the train and test set was decreasing in a pretty 
stable manner. Using this network, we achieved for the first time a MSE of under 10 on the test set,
which is, according to our assumptions in \cref{subsec:AimAndMeasure}, a 
pretty good result --- especially, as we trained the model mostly on highway scenes and tested it only 
in city driving scenarios.
\begin{table}[!t]
\normalsize
\centering
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{Initial splitting, 8 epochs}  & \multicolumn{2}{c}{$\mathrm{ReLU}$} & \multicolumn{2}{c}{$\mathrm{leakyReLU}$} \\
 & Train & Test & Train & Test\\
\midrule
No pooling & 2.85 & 12.08 & 2.45 & 10.75 \\
Max pooling & 5.62 & 11.82 & 5.52 & 10.29 \\
Max pooling (15 epochs) & - & - & \textbf{3.22} & \textbf{9.63} \\
Average pooling & 7.70 & 11.40 & 6.08 & 13.09\\
\bottomrule
\end{tabular}
\caption{MSE results of the network using different pooling strategies, one dropout layers, two different activation functions and 
the initial splitting. We trained each of the models for eight epochs.}
\label{tab:ResultsInitialSplitting}
\end{table}

\subsection{Siamese approach}

\section{Training process}

\subsection{Optimizer and scheduler}
For all of our networks we used \emph{ADAM} \cite{Adam2014} as an optimizer. SCHEDULER

%\begin{comment}
%\subsection{New splitting and siamese network}
%We decided to take a closer look at the different road traffic scenarios in the video and segmented the 
%frames into three category. The first category
%consists of highway driving scenes (from minute 0:00 to minute 7:30), the second one contains frames of 
%the car being
%in a stop and go scenario on the highway (from minute 7:31 to minute 15:00) and the third category 
%consists of
%city driving scenes (from minute 15:01 to the end). We visualized the velocity distribution and the 
%categories in 
%\cref{fig:SpeedPerFrameDistributionNewSplitting}. 
%We shuffled the frames of each block randomly and used again the 80/20 rule to split each of the 
%category blocks into train and test data.
%\end{comment}



\section{Error analysis and results}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.99\columnwidth]{imgs/TrainingProcess.eps}
	\caption{Comparison of the results using the models: tuned original network and siamese model with frames and optical flow; \textit{top}: training and validation error per epoch; \textit{bottom}: predictions after the last epoch with correct values in red}
	\label{fig:resultsSummary}
\end{figure}

We identified three possible reasons for our poor results:
\begin{enumerate}[label=(\roman*)]
	\item predictions are limited to specific video parameters: predictions are bound to camera perspective, changes in brightness
	\item lack of generalization: perfect fitting on randomly shuffled validation data, qualitative fitting for block splitting and rough fitting for different videos
	\item very complex model trained with very little data
\end{enumerate}



%\begin{enumerate}
%	\item[(i)] Too complex model, as the paper used it for autonomous driving or we put too little information into the model.
%	\item[(ii)] Problems with different brightnesses in the frames (lack of generality), which leads to unstable calculations in the 
%	optical flow, as the optical flow is quite sensitive to brightness. (Explain how in the train part the sky is quite dark and in the 
%	city (end) the sky is bright)
%	\item[(iii)] Too naive/ambiguous splitting of the data into train and testing set, as both datasets seem to represent totally different
%	scenarios in the road traffic.
%\end{enumerate}
We came up with the following approaches to solve these problems
\begin{enumerate}[label=(\roman*)]
%	\item Simplify the model by using Pooling (we will try average and max pooling), to get more compression and we tried on the
%	other hand to feed in more information into the model, by using a linear combination of the optical flow and the raw frames itself, and
%	we tried using a siamese network, to put simultaneously the of and raw frames into the convolutional layers.
	\item We wanted to try adding some additional noise into the frames before calculating the optical flow, to make the calculation
	more robust against brightness changes. As intentionally adding 
	noise to a frame is quite atypical in computer vision, this idea looked quite interesting.
	\item Use another splitting. To get a better ratio between highway and city driving scenarios, we decided to split the data into
	blocks of 100 frames and take the first 80 for training and the last 20 for testing. Therefore our model should have seen some city
	driving.
\end{enumerate}




\section{Further ideas for improvements}

\subsection{Augmented brightness}
STILL UNDER CONSTRUCTION

\subsection{Increase data}

Use MUCH MUCH MUCH MUCH MUCH more data!

Create Validation Data that is not connected to training data and still covers all situations


\section*{Acknowledgment}
We like to thank bla bla bla




\printbibliography

\end{document}
