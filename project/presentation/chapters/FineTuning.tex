\section{Fine-tuning of the model}
\subsection{Initial tuning}

\begin{frame}{Batch Normalization, Dropout layers, activation function and pooling}
\begin{itemize}
\item Batch normalization to speed up the training \cite{BatchNorm2015}
\item Initial activation function: $\mathrm{ReLu}: \mathbb{R} \to \mathbb{R}_0^+, x \mapsto \max\{0,x\}$, still MSE over 15 on the testing set\\
$\Rightarrow$ Overfitting problems
\item Found paper about dropout layers \cite{Dropout2014} to reduce overfit
%build in one with dropout probability $p=0.5$
\item Solve problems of dead neurons using
\begin{align*}
\mathrm{leakyReLU} : \mathbb{R} \to \mathbb{R}, x \mapsto \begin{cases}
x, x \geq 0\\
c \cdot x, x <0
\end{cases}
\end{align*}
with $c = 0.01$, MSE of around 11 on the testing set
\end{itemize}
\end{frame}
\subsection{Problems and possible solutions}
\begin{frame}{Problems}
We identified three possible problems for poor results
\begin{enumerate}
\item Too complex model, as initially used for autonomous driving or insufficient amount of information put into the model
\item Problems with different brightnesses/illumination changes in the frames, therefore unstable calculations of the optical flow
\item Too ambiguous splitting, as the training and testing datasets represent totally different road traffic scenarios in the road 
traffic
\end{enumerate}
IMAGE OF THE SPEED DISTRIBUTION
\end{frame}
\begin{frame}{Possible solutions}

\begin{enumerate}
\item \textbf{Simplify model}: pooling layers (maximum and average pooling) to get more compression\\
\textbf{Siamese approach}: put flow field and raw frame into the model or put two consecutive frames into the model
\item \textbf{Add additional noise}: add noise before computing the optical flow filed, to get more invariance regarding illumination changes
\item \textbf{Different splitting}: get better ratio between different scenarios, by using different data splittings:
finer one and a more specific one based on the different road traffic situations in the video
\end{enumerate}
\end{frame}
\subsection{Simplified model}
\begin{frame}{1.1 Results using pooling layers}
\begin{table}[!t]
\normalsize
\centering
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{Initial splitting, 8 epochs}  & \multicolumn{2}{c}{$\mathrm{ReLU}$} & \multicolumn{2}{c}{$\mathrm{leakyReLU}$} \\
 & Train & Test & Train & Test\\
\midrule
No pooling & 2.85 & 12.08 & 2.45 & 10.75 \\
Max pooling & 5.62 & 11.82 & 5.52 & 10.29 \\
Max pooling (15 epochs) & - & - & \textbf{3.22} & \textbf{9.63} \\
Average pooling & 7.70 & 11.40 & 6.08 & 13.09\\
\bottomrule
\end{tabular}
\caption{MSE results of the network using different pooling strategies, one dropout layers, two different activation functions and 
the initial splitting. We trained each of the models for eight epochs.}
\end{table}
\end{frame}
\subsection{New splitting}
\begin{frame}{1.2 Siamese approach with new splitting}
RESULTS ARE NEEDED :)
\end{frame}
