\documentclass[a4paper, headsepline]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{lastpage}
\usepackage{units}

\usepackage{amsmath}%
%\usepackage{MnSymbol}%
%\usepackage{wasysym}%

\usepackage[left=3cm,right=3cm,top=1.5cm,bottom=1.5cm,includeheadfoot]{geometry}

\usepackage{graphics}

\usepackage[colorlinks, linkcolor = black, citecolor = black, filecolor = black, urlcolor = blue]{hyperref}

\usepackage{graphics}
\def\R{{\mathbb{R}}}
\def\N{{\mathbb{N}}}
\def\C{{\mathbb{C}}}
\def\K{{\mathbb{K}}}
\def\Q{{\mathbb{Q}}}
\def\Z{{\mathbb{Z}}}
\def\O{{\mathcal{O}}}
\def\Pot{{\mathcal{P}}}
\def\P{{\mathbb{P}}}
\def\D{{\mathcal{D}}}
\def\B{{\mathcal{B}}}
\def\U{{\mathcal{U}}}
\def\F{{\mathcal{F}}}
\def\M{{\mathcal{M}}}
\def\E{{\mathbb{E}}}
\def\EEE{{\mathcal{E}}}
\def\Cov{{\mathbb{C}\mathrm{ov}}}
\def\Var{{\mathbb{V}\mathrm{ar}}}
\def\V{{\mathbb{V}}}
\def\ND{{\mathcal{N}}}
\def\essup{{\mathrm{ess \;sup}}}

\usepackage{csquotes}

\usepackage[autooneside]{scrlayer-scrpage}
\pagestyle{empty}
\clearscrheadfoot
\ihead{Florian Wolf}
\ohead{Machine Learning using Matlab}
\ifoot{\today}
\ofoot{\thepage\ / \pageref{LastPage}}
\pagestyle{scrheadings}

\newcommand{\up}[1]{\overset{#1}{\Leftrightarrow}}
\newcommand{\lra}{\Leftrightarrow}
\newcommand{\ra}{\rightarrow}

\usepackage{aligned-overset}

\usepackage{xpatch}
\xpatchcmd{\proof}{\itshape}{\normalfont\proofnamefont}{}{$\square$}

\newcommand{\proofnamefont}{\underline}

\newcommand*\dx{\: \mathrm{d}x}
\newcommand*\dd{\: \mathrm{d}}
\newcommand*\dy{\: \mathrm{d}y}
\newcommand*\dt{\: \mathrm{d}t}
\newcommand*\du{\: \mathrm{d}u}
\newcommand*\da{\: \mathrm{d}a}
\newcommand*\dv{\: \mathrm{d}v}
\newcommand*\dr{\: \mathrm{d}r}
\newcommand*\dz{\: \mathrm{d}z}
\newcommand*\dOp{\: \mathrm{d}}
\newcommand*\dmu{\: \mathrm{d}\mu}
\newcommand*\dlambda{\: \mathrm{d}\lambda}
\newcommand*\dP{\: \mathrm{d}\mathbb{P}}
\newcommand*\dth{\: \mathrm{d}\Theta}


% komplexe Zahl
\def\e{{\mathrm{e}}}
\def\mi{{\mathrm{\textbf{i}}}}
\newcommand{\conj}[1]{%
  \overline{#1}%
}

\usepackage[capitalise]{cleveref} %make first letter a capital one
\usepackage{enumerate}

\newcommand{\scpr}[2]{\langle #1,#2 \rangle_2 }
\newcommand{\EE}[1]{\E\left[#1\right]}
\newcommand{\PP}[1]{\P\left(#1\right)}
\newcommand{\VVar}[1]{\Var\left[#1\right]}
\newcommand*{\set}[1]{\left\{#1\right\}}
\newcommand*{\ind}[1]{\mathds{1}_{#1}}


\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{dsfont}

\usepackage{thmtools}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\normLp}[1]{\left\lVert#1\right\rVert_{L^p}}
\newcommand{\normLq}[1]{\left\lVert#1\right\rVert_{L^q}}
\newcommand{\normLinf}[1]{\left\lVert#1\right\rVert_{L^\infty}}
\newcommand{\normlp}[1]{\left\lVert#1\right\rVert_{l^p}}
\newcommand{\normlq}[1]{\left\lVert#1\right\rVert_{l^q}}
\newcommand{\normlinf}[1]{\left\lVert#1\right\rVert_{l^\infty}}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\normLone}[1]{\left\lVert#1\right\rVert_{L^1}}


\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{satz}[theorem]{Hilfssatz}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Korollar}
\begin{document}
\section*{Exercise 3}
\subsection*{Question 1}
\underline{Relation between $\varepsilon$ and noise variance:} We assume that the error in the data is normally distributed with mean 
$\mu$ and variance $\sigma^2$. Then choosing $\varepsilon \approx \sigma^2$ is the optimal choice, because if we take a closer look
at the formula on slide 16 (lecture 5), we can see that our due to our choise of $\varepsilon$ the loss function will accept results
which differences are smaller than $\sigma^2$. This makes sense, because this is the variance (e.g. the standard deviation).

The assumption of a normal distributed error in the data is, considering a practical case, relatively good as we can see using the 
central limit theorem or the (strong) law of large numbers.\\
\underline{$\varepsilon$ vs. over-/underfitting:} We differ the cases
\begin{itemize}
\item $\varepsilon$ small: The decision boundary is quite hard and the model does not accept a lot of noise in the data, so our model
has a tendency to overfitting.
\item $\varepsilon$ big: The model accepts way more noise and in the edge case ($\varepsilon$ being so big, that the real margin is 
smaller than epsilon) the model accepts the whole data as one class. This a tendency to underfitting.
\end{itemize}
\subsection*{Question 2}
We define $L(w,b):= \frac{1}{2} \norm{w}^2 + \sum_{i=1}^m \hat{\xi}^{(i)} + \xi^{(i)}$ and we want to minimize $L$ with respect to 
the following constraints
\begin{align*}
y^{(i)} - (w\Phi(x^{(i)} +b) &\leq \varepsilon +  \hat{\xi}^{(i)}\\
(w\Phi(x^{(i)}) +b)-y^{(i)} &\leq \varepsilon + \xi^{(i)}\\
\hat{\xi}^{(i)} \geq 0; \xi^{(i)} &\geq 0, \forall i \in \{1,\ldots, m\}
\end{align*}
We want to find the dual form of this minimization problem using Lagrange multipliers. Therefore we define for $i=1,\ldots, m$
the function
\begin{align*}
f^{(i)}(w,b) := \begin{pmatrix}
y^{(i)} - (w\Phi(x^{(i)}) +b) - \varepsilon -  \hat{\xi}^{(i)}\\
(w\Phi(x^{(i)}) +b)-y^{(i)} - \varepsilon - \xi^{(i)}\\
-\hat{\xi}^{(i)}\\
-\xi^{(i)}
\end{pmatrix}
\end{align*}
which, regarding our constraints, should be less or equal to zero for each component. Regarding the partial derivatives, we get
\begin{align*}
\nabla_w f^{(i)} =\begin{pmatrix}
-\Phi(x^{(i)}) \\
\Phi(x^{(i)})\\
0\\
0
\end{pmatrix}, \nabla_{b}f^{(i)} = \begin{pmatrix}
-1\\
1\\
0\\
0
\end{pmatrix}, \nabla_{\hat{\xi}^{(j)}}f^{(i)} = \delta_{ij}\cdot (0,-1,0,1)^T, \nabla_{\xi^{(j)}}f^{(i)} = (-1,0,-1,0)^T.
\end{align*}
Now, we use Lagrange multipliers to obtain the ansatz
\begin{align*}
F(w,b, ((\lambda_k^{(i)})_{\substack{k=1,\ldots, 4\\i=1,\ldots, m}}) = L(w,b) + \sum_{i=1}^m \sum_{k=1}^4 \lambda_k^{(i)} f_k^{(i)}(w,b).
\end{align*}
We now compute the partial derivatives, set them to zero and therefore obtain
\begin{align*}
0&= \nabla_w F = w + \sum_{i=1}^m \sum_{k=1}^4 \lambda_k^{(i)} \nabla_w f_k^{(i)}(w,b) = w + \sum_{i=1}^m -\lambda_1^{(i)} \Phi(x^{(i)}) 
+ \lambda_2^{(i)} \Phi(x^{(i)})\\
0&= \nabla_b F = \sum_{i=1}^m - \lambda_1^{(i)} + \lambda_2^{(i)}\\
0&= \nabla_{\hat{\xi}^{(j)}} F = C - \lambda_1^{(j)} - \lambda_3^{(j)}\\
0&= \nabla_{\xi^{(j)}} F = C - \lambda_2^{(j)} - \lambda_4^{(j)}.
\end{align*}
If we put this back into $F$, we obtain the dual form. Using the first of the upper equations, we have
\begin{align*}
w &= \sum_{i=1}^m \lambda_1^{(i)} \Phi(x^{(i)}) - \lambda_2^{(i)} \Phi(x^{(i)})\\
\Rightarrow \frac{1}{2} \norm{w}^2 &= \frac{1}{2} w^T w = \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m (\lambda_1^{(i)} - \lambda_2^{(i)})
(\lambda_1^{(j)} - \lambda_2^{(j)}) \Phi(x^{(i)})^T \Phi(x^{(j)})
\end{align*}
Now we infer
\begin{align*}
F(w,b, ((\lambda_k^{(i)})_{\substack{k=1,\ldots, 4\\i=1,\ldots, m}}) &= \frac{1}{2} \norm{w}^2 + \underbrace{C \sum_{i=1}^m (\hat{\xi}
^{(i)} \xi^{(i)}) + \sum_{i=1}^m \lambda_3^{(i)}  (-\hat{\xi}^{(i)}) + \sum_{i=1}^m \lambda_4^{(i)}  (-\xi^{(i)})}_{= \sum_{i=1}^m
(C-\lambda_3^{(i)}) \hat{\xi}^{(i)} + (C-\lambda_4^{(i)}) \xi^{(i)} = \sum_{i=1}^m
\lambda_1^{(i)} \hat{\xi}^{(i)} + \lambda_2^{(i)} \xi^{(i)}}\\
&+\sum_{i=1}^m\lambda_1^{(i)} f_1^{(i)}(w,b) + \sum_{i=2}^m\lambda_1^{(i)} f_2^{(i)}(w,b)\\
&= \frac{1}{2} \norm{w}^2 + \underbrace{\sum_{i=1}^m \lambda_1^{(i)} (f_1^{(i)}(w,b) + \hat{\xi}^{(i)})}_{\sum_{i=1}^m \lambda_1^{(i)}
(y^{(i)} - (w\Phi(x^{(i)}) +b) - \varepsilon)}
+ \underbrace{\sum_{i=1}^m \lambda_2^{(i)} (f_2^{(i)}(w,b) + \xi^{(i)}}_{\sum_{i=1}^m \lambda_2^{(i)}((w\Phi(x^{(i)}) +b)-y^{(i)} - 
\varepsilon)}\\
&= \frac{1}{2} \norm{w}^2 + \underbrace{\sum_{i=1}^m (- \lambda_1^{(i)} + \lambda_2^{(i)})}_{=0}b 
+ (-\varepsilon)\sum_{i=1}^m  \lambda_1^{(i)} + \lambda_2^{(i)} + \sum_{i=1}^m (- \lambda_1^{(i)} + \lambda_2^{(i)})y^{(i)}\\
&+\underbrace{\sum_{i=1}^m (-\lambda_1^{(i)} + \lambda_2^{(i)})w \Phi(x^{(i)})}_{=-w^T w, \text{ using the first constraint}}
\end{align*}
Therefore we get the desired result for the dual form
\begin{align*}
F(w,b, ((\lambda_k^{(i)})_{\substack{k=1,\ldots, 4\\i=1,\ldots, m}}) &= - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m (\lambda_1^{(i)} - 
\lambda_2^{(i)})(\lambda_1^{(j)} - \lambda_2^{(j)}) \Phi(x^{(i)})^T \Phi(x^{(j)}) + (-\varepsilon)\sum_{i=1}^m  \lambda_1^{(i)}
+ \lambda_2^{(i)}\\ &+ \sum_{i=1}^m (- \lambda_1^{(i)} + \lambda_2^{(i)})y^{(i)}.
\end{align*}
This was our claim.
\end{document}